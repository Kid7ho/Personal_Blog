<!DOCTYPE html>
<html>
    <head>
        <title>Personal Blog</title>
        <link rel = "stylesheet" type = "text/css" href = "../styles/main.css">
        <link rel = "stylesheet" type = "text/css" href = "../styles/post.css">
        <script src = "../scripts/article.js"></script>
    </head>

    <body>
        <header>
            <div class = "navbar">
                <span class = "logo"><a href = "../index.html">JIMMY</a></span>
                <nav>
                    <ul>
                        <a href = "../index.html"><li>Home</li></a>
                        <a href = "../articles.html"><li><strong>Articles</strong></li></a>
                        <a href = "../projects.html"><li>Projects</li></a>
                        <a href = "../about.html"><li>About</li></a>
                        <a href = "../contact.html"><li>Contact</li></a>
                    </ul>
                </nav>
            </div>
        </header>
        
        <main>
            <section id = "contents">
                <div class = "title" style = "background-image: url(../images/articles/research_lm-implementation/main.jpg);">
                    <button class = "return" type = "button" onclick = "location.href = '../articles.html'">&lt; Get Back to Articles</button>
                    <div class = "text">Analyzing the Performance Difference of Language Model with Various Implementation Methods</div>
                </div>
                
                <div id = "content">
                    <div class = "info">
                        <strong><a href = "../about.html">Jin-Myoung Hyun</a></strong> · Research / Articifial Intelligence · 2023.11.24
                    </div>
                    
                    <hr>

                    <p class = "section">1. Introduction</p>
                    <p>There are many ways to implement Language Models and the way of implementing Language Models affects Language Models' reasoning abilities. In this article, we will actually try different kinds of implementations of Language Models and will compare the performances of each implementation to find out how we can improve Language Models' reasoning abilities.</p>
                    <p>In this article, we will use OPT-350m, OPT-1.3B, OPT-2.7B, and LLaMA2-7B which are provided through API running on the lab server. As the dataset for our experiment, we will load the GSM8K dataset and use that for training (Check Step 0-2. of the attached file for more detail and for actual source code).</p>
                    <p>Note that in this article, when I refer to 'Step \#-\#.', that means to check that particular step in the attached file of "Assignment-3_SourceCode_2022148033.ipynb", and when I refer to '#-#. xxxx', which means to check that particular part in this document.</p>

                    <hr>

                    <p class = "section">2. First Experiment: Zero-shot Prompting vs Few-shot Prompting</p>
                    <p class = "subsection">2.1. Planning Experiment</p>
                    <p>For our first experiment, we will check the performance difference between Zero-shot Prompting and Few-shot Prompting. In order to check whether Language Models' reasoning ability improves proportionally to the number of shots, we will implement Zero-shot, Five-shot, Ten-shot and calculate the accuracy of the models, respectively.</p>
                    <p>In order to implement Zero-shot, Five-shot, and Ten-shot Prompting, we will run the source code in Step 1-1. Since in these prompting, we will not implement Chain-of-Thought. Therefore, we will only split the actual answer from the dataset using the 'split("\#\#\#\#")[-1]' function. By calling the 'construct\_prompt()' function, we made Zero-shot, Five-shot, and Ten-shot Prompting models. Then we will run source codes on Step 1-2., Step 1-3., Step 1-4. to calculate the accuracy of each model. We will run 50 inputs on each model and since running these models only once can lead us to do hasty generalization, we will repeat the experiment 5 times.</p>
                    <p class = "subsection">2.2. Experiment Result</p>
                    <p>The following is the result of the experiment:</p>
                    <p>Table 1</p>
                    <p>You can check the prompt of each model and the actual result of each model on the attached files (prompt\_0shot.txt, result\_0shot\_direct.json, etc.). For the actual result, even though we have run the same source code 5 times, only the last one is attached.</p>
                    <p>Let's analyze the result. By checking the average accuracy each model produced, we can see that Few-shot prompting does improve reasoning ability compared to Zero-shot prompting. Also, we can check that increasing the number of shots may not always proportionally increase reasoning ability. However, we can not generalize that as the number of shots increases, the reasoning ability stays the same due to the reasons that will be stated later on (Check 2.3. Limitations).</p>
                    <p class = "subsection">2.3. Limitations</p>
                    <p>Let's think about the limitations of the experiment. First of all, as stated above, we repeated the experiment 5 times to avoid hasty generalization. However, 5 times of repetition may also not be enough to avoid hasty generalization. If we repeat the experiment more than 100 times, it may change the result. In the same sense, we only tested 50 inputs per model, which will not be enough amount of inputs.</p>
                    <p>Secondly, the limitation is that we only used the GSM8K dataset. Since only one kind of dataset was being used, the model trained might have been overfitted by the dataset.</p>
                    <p>Lastly, the process of splitting the answer from the result from each model made some errors. For example, if we check 'result\_5shot\_direct.json', the prediction extracted for question \#3 is '\$250,000'. Also, for question \#15, the prediction extracted is '25\%'. Since the answer stored in the dataset is just an integer that doesn't have any measurement in front or after the number and it also doesn't have a 'comma' between every 3 digits. Therefore, even if the actual prediction was correct while comparing the prediction with the actual answer, the source code might have assumed such a case 'wrong' which makes the accuracy of the model lower than the actual. Therefore, those are the limitations of Experiment 1.</p>

                    ...

                    <hr>

                    
                </div>

                <div id = "search">
                    <div class = "category"></div>
                    <div class = "next_article"></div>
                    <div class = "recent">최근 글:</div>
                    <div class = "new_article"></div>
                    <ul class = "tag">
                        <li>#Artificial Intelligence</li>
                        <li>#Language Model</li>
                        <li>#Chain of Thought</li>
                        <li>#Zero Shot Prompting</li>
                        <li></li>
                    </ul>
                </div>
            </section>
        </main>

        <footer>
            <div class = "logo">JIMMY</div>

            <p>@Copyright 2023 Jin-Myoung Hyun. All rights reserved.</p>
            <br>
            <p>Address: 155, Dongpangyo-ro, Bundang-gu, Seongnam-si, Gyeonggi-do, Republic of Korea</p>
            <p>E-mail: jinmyoung.hyun@gmail.com</p>
            <p>Tel: 010-4015-3961</p>
        </footer>
    </body>
</html>